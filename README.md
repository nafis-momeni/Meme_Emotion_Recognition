# Meme Emotion Recognition Project

## Introduction

Welcome to the Meme Emotion Recognition Project! This repository serves as a learning journey into the realm of multimodal machine learning. Emotions in memes are complex, often wrapped in humor, sarcasm, offense, and motivation. The primary goal of this project is to explore multimodal concepts, combining both textual and visual information to recognize emotions within memes.
## Data Source

To streamline dataset download from Kaggle within the project, you'll need to set up Kaggle authentication using your `kaggle.json` file linked to your Kaggle account. Here's how:

- The Kaggle dataset page [here](https://www.kaggle.com/williamscott701/memotion-dataset-7k).
- Log in to your Kaggle account.
- Go to your Kaggle account settings [here](https://www.kaggle.com/account).
- Scroll down to the "API" section.
- Click "Create New API Token." This downloads a file called `kaggle.json`.
- Place the `kaggle.json` file in the content/ directory of the notebook.


## Models and Methods

This project offers an exploration into multimodal learning methods, including:

- **Bidirectional LSTM (BiLSTM):** A recurrent neural network model for sequential data analysis.
- **RoBERTa:** A transformer-based model showcasing the capabilities of text analysis.
- **ResNet:** A convolutional neural network (CNN) designed for image recognition, despite challenges posed by meme data's uniqueness.
- **Multimodal Fusion Techniques:** The project delves into methods such as Early Fusion and Gated Multimodal Units (GMU) to combine textual and visual information.



## Contributing

If you'd like to contribute to this project or have any questions about the models and methods used, please feel free to open an issue or reach out. Your contributions are welcomed and appreciated.

